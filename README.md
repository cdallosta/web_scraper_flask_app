# Web Scraping/Flask Applications

## Summary

The following directory contains a web scraping application, a flask application and a postgres database.
All solutions are dockerized. 

### Web Scraper

The web scraping application scrapes the following url, locating the correct html element in order to
 click and download an excel data set:
- "https://www.pjm.com/planning/services-requests/interconnection-queues.aspx"

Upon download the application cleans the dataset and extracts the county and state for each project contained
in the downloaded excel file. The application then queries the opencagedata api in order to extract the latitude
and longitude. The pjm data and the location data is then merged and uploaded to the postgres database


### Flask App
The flask application functions as an interfacce for the postgres database. The api can be queried by county, state,
or county AND state. The endpoint only accepts GET request, takes in url parameters and is case insensitive.


GET Endpoint Configuration:
- Filter by county
    - http://{host}:5000/pjm_projects?county={county_name}
- Filter by state
    - http://{host}:5000/pjm_projects?state={state_name}
- Filter by county and state
    - http://{host}:5000/pjm_projects?county={county_name}&state={state_name}

## SETUP
### Assumptions:
- Docker is installed on the host device
- The run.sh file is required to be run on a linux os

### Steps
Option 1:
1. Execute the run.sh file
    - . run.sh

Option 2:
1. Execute the following cli command:
    - docker-compose build && docker-compose up -d

***NOTE***:

The application should take no longer than a few minutes to download all dependencies and deploy
